{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nodeblackbox/Faceted_judge_2022_08_29_03_31_59/blob/master/lab_5_BOSTON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gb6X1pzNp1"
      },
      "source": [
        "# The Boston Housing Dataset (or California)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Practice\n",
        "\n",
        "- Run k-fold validation on the Boston dataset;\n",
        "- Notice that the mini-batch size is set to 1. Experiment with different mini-batch sizes. What do you observe? Can you account for your observation?\n",
        "- Run a series of experiments to find the best model. (Hint: look back at the previous labs.)\n",
        "- Retrain the best model on all the training data and evaluate on the test data.\n",
        "\n",
        "#### Note\n",
        "\n",
        "For those who wish to work on the California Housing Dataset instead, follow the instructions at the bottom of the notebook. This counts 'not covered' dataset in the first coursework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "81AjEF2BnvFS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQvMWYSdzNp8",
        "outputId": "abbe6daa-695f-4004-ceec-28d6d483d0b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JSFfgLRQnvFc"
      },
      "outputs": [],
      "source": [
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean # shift\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std # rescale\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXY1e_6HnvFd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            64, activation = 'relu', input_shape = (train_data.shape[1], )\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dense(64, activation = 'relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bOCOzAjOzNqC"
      },
      "outputs": [],
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, epochs):\n",
        "        super(tf.keras.callbacks.Callback, self).__init__()\n",
        "        self.epochs = epochs\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        c = ['|', '/', '-', '\\\\'] \n",
        "        print(f\"\\r{c[epoch % 4]} epoch: {epoch+1}/{self.epochs}\", end=\"\")\n",
        "    def on_train_end(self, logs=None):\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cubqyRFzNqG",
        "outputId": "651f85d8-55c3-4d56-dbed-41ac8182fddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold 0\n",
            "/ epoch: 242/500"
          ]
        }
      ],
      "source": [
        "K = 4\n",
        "num_val_samples = len(train_data) // K\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(K):\n",
        "    print('processing fold', i)\n",
        "    \n",
        "    # Prepare the validation data: data from partition i\n",
        "    a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
        "    val_data = train_data[a : b]\n",
        "    val_targets = train_targets[a : b]\n",
        "    \n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate([train_data[:a], train_data[b:]], axis=0)\n",
        "    partial_train_targets = np.concatenate([train_targets[:a], train_targets[b:]], axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    \n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    history = model.fit(\n",
        "        partial_train_data,\n",
        "        partial_train_targets,\n",
        "        validation_data=(val_data, val_targets),\n",
        "        epochs=num_epochs, batch_size=1, verbose=0, \n",
        "        callbacks=[CustomCallback(num_epochs)]\n",
        "    )\n",
        "\n",
        "    mae_history = history.history['val_mae']\n",
        "    all_mae_histories.append(mae_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dNFOVfdzNqJ"
      },
      "outputs": [],
      "source": [
        "average_mae_history = np.array(all_mae_histories).mean(axis=0)\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I58FoiazNqM"
      },
      "outputs": [],
      "source": [
        "def smooth_curve(points, beta = 0.9):       # beta must be between 0 and 1!\n",
        "    smoothed_points = []\n",
        "    for current in points:\n",
        "        if smoothed_points:                 # (an nonempty list is 'True')\n",
        "            previous = smoothed_points[-1]  # the last appended point\n",
        "                                            # â†“ a weighted sum of previous & point, controlled by beta\n",
        "            smoothed_points.append(beta * previous + (1 - beta) * current)\n",
        "        else:\n",
        "            smoothed_points.append(current) # at the start, the list is empty, we just add the first point\n",
        "    return smoothed_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IaEOcb8zNqO"
      },
      "outputs": [],
      "source": [
        "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
        "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYzJuSdnnvGS"
      },
      "source": [
        "---\n",
        "\n",
        "# Optional\n",
        "\n",
        "## The California Housing dataset\n",
        "\n",
        "[California Housing](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), original website. (Also available on [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).)\n",
        "\n",
        "### 1. Download\n",
        "\n",
        "The terminal commands to download it. (Add a `!` in front of them to use them from Jupyter or Colab.)\n",
        "\n",
        "```bash\n",
        "wget https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\n",
        "tar -xvf cal_housing.tgz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZuWksnKnvGT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hcnoWudnvGT"
      },
      "source": [
        "### 2. Load the data\n",
        "\n",
        "Use the name of the file `cal_housing.data` to:\n",
        "- open it \n",
        "- read the lines \n",
        "- strip the final newline `\\n` \n",
        "- split on commas\n",
        "- turn the data into a numpy array, casting it as floats\n",
        "\n",
        "#### Note\n",
        "\n",
        "You can see the features by loading `cal_housing.domain`, read its lines, and print its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WUb_PRrnvGU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAypodSTnvGU"
      },
      "source": [
        "## 3. Separate the features and the targets\n",
        "\n",
        "The price is the last feature, so you need to use NumPy to slice all the `targets` in the last dimension, and the rest of the `features` in another array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sZM4ZZCnvGV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKx3qv2DnvGV"
      },
      "source": [
        "## 4. Scale the prices to a more manageable range\n",
        "\n",
        "You can print the `min()` and the `max()` of your `targets` to see the kind of range we are dealing with.\n",
        "\n",
        "Then a division by `100000` will give us similar numbers to the Boston Housing Dataset.\n",
        "\n",
        "Once you have your reduced targets, you may want to print again the `min()` and the `max()` as a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocwyY2TdnvGV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfhWzWvznvGW"
      },
      "source": [
        "## 5. Split your data into train and test\n",
        "\n",
        "Use `.shape` on your `features` (and/or `targets`) to check how many samples this dataset has.\n",
        "\n",
        "Slice both `features` and `targets` to obtain `train_data`, `test_data`, and `train_targets`, `test_targets` respectively.\n",
        "\n",
        "This is actually a potential subject of experiment. You could slice it roughly in the middle, or have more in your training than your testing set.\n",
        "\n",
        "As a sanity check, your shapes should look like this:\n",
        "```\n",
        "# n_train: number of training samples\n",
        "# n_train: number of testing samples\n",
        "train_data (n_train, 8)\n",
        "train_targets (n_train,)\n",
        "test_data (n_test, 8)\n",
        "test_targets (n_test,) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIZogMMenvGe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMKF4JvpnvGh"
      },
      "source": [
        "## 6. Normalisation/scaling\n",
        "\n",
        "Use the mean and standard deviation of the **train data** to normalise it, and apply the same transform to test data, exactly as above with the Boston Housing Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvyPt5JcnvGi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T-0Zv1VnvGi"
      },
      "source": [
        "## 7. Everything is now set up for training\n",
        "\n",
        "The rest of the procedure (define the model, train, plot) is now the same.\n",
        "\n",
        "#### Note\n",
        "\n",
        "This dataset is not small like the Boston Housing Dataset, so you may find that it's taking too long to do many epochs with K-fold given the compute you have. This doesn't matter *too* much, the important thing is to understand the K-fold logic.\n",
        "\n",
        "#### Experiment\n",
        "\n",
        "You could artificially reduce your training set to a similar size as the Boston Housing Dataset (~400 samples), and see what performance you manage to get on your (then much, much larger) test set!\n",
        "\n",
        "#### Visualisations\n",
        "\n",
        "Three examples of how people use visualisations for this dataset:\n",
        "- [California Housing Modelling and Map Visualisation](https://www.kaggle.com/code/qixuan/california-housing-modelling-and-map-visualisation)\n",
        "- [California Housing Prices: EDA and Visualization](https://www.kaggle.com/code/ujwalkandi/california-housing-prices-eda-and-visualization)\n",
        "- [The California housing dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}